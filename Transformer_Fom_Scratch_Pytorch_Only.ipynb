{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3ce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python_3\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\python_3\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\python_3\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\python_3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\python_3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python_3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\python_3\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\python_3\\lib\\site-packages (from torch) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python_3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python_3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python_3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python_3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python_3\\Lib\\site-packages)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(d_model,vocab_size, padding_idx=padding_idx)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)*math.sqrt(self.embedding.embedding_dim)\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dd8c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        %Create a matrix of shape (seq_len, d_model) to hold the positional encodings\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)) #Implemented the eqn in log for ease of computation\n",
    "        %Apply the sine function to even indices in the array; 2i\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        %Apply the cosine function to odd indices in the array; 2i+1            \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)    \n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Shape (1, seq_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.size(1), :]).require_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8a3a1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  LayerNorm(nn.Module):\n",
    "    def __init__(self, eps=10e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha=nn.Parameter(torch.ones(1))#Multiplied\n",
    "        self.bias=nn.Parameter(torch.zeros(1))#Addeed\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean=x.mean (-1, keepdim=True)\n",
    "        std=x.std (-1, keepdim=True)\n",
    "        return self.alpha*(x-mean)/(std+self.eps)+self.bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "924148e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1=nn.Linear(d_model,d_ff)#W1 ansd B1\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.linear2=nn.Linear(d_ff,d_model) #W2 and B2\n",
    "        self.relu=nn.ReLU() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d435bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__ (self, d_model, h, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
    "        \n",
    "        self.d_k = d_model//h\n",
    "        self.w_q=nn.Linear(d_model,d_model) #Wq\n",
    "        self.w_k=nn.Linear(d_model,d_model) #Wk\n",
    "        self.w_v=nn.Linear(d_model,d_model) #Wv\n",
    "        \n",
    "        self.w_o=nn.Linear(d_model,d_model) #Wo\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        \n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=None):\n",
    "        d_k=query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)  # Scaled dot-product attention\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(torch.softmax(attention_scores, dim=-1))\n",
    "            \n",
    "        return (attention_scores @ value), attention_scores  # (batch_size, h, seq_len, d_k), (batch_size, h, seq_len, seq_len)\n",
    "    \n",
    "    def forward(self,query, key, value,mask):\n",
    "        query=self.w_q(query) # (batch_size, seq_len, d_model)\n",
    "        key=self.w_k(key)     # (batch_size, seq_len, d_model)\n",
    "        value=self.w_v(value) # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        query=query.view(query.size(0), -1, self.h, self.d_k).transpose(1,2) # (batch_size, h, seq_len, d_k)\n",
    "        key=key.view(key.size(0), -1, self.h, self.d_k).transpose(1,2)         # (batch_size, h, seq_len, d_k)\n",
    "        value=value.view(value.size(0), -1, self.h, self.d_k).transpose(1,2)     # (batch_size, h, seq_len, d_k)\n",
    "\n",
    "        #batc   h_size, h, seq_len, d_k--> batch, seq_len, d_model\n",
    "        x = x.transpose(1, 2).contiguous().view(x.size(0), -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.w_o(x)  # (batch_size, seq_len, d_model) \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31f1ca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.norm=LayerNorm()\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        return (x + self.dropout(sublayer(self.norm(x)))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b7a4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Block(nn.Module):\n",
    "    def __init__(self, self_attention_block=MultiHeadAttention, feed_forward_block=FeedForward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.feed_forward_block=feed_forward_block\n",
    "        self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "        \n",
    "    def forward(self,x,src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d97319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.norm=LayerNorm()\n",
    "\n",
    "def forward(self, x, src_mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,src_mask)\n",
    "        return self.norm(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "099e0b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Block(nn.Module):\n",
    "    def __init__(self,self_attention_block=MultiHeadAttention,cross_attention_block=MultiHeadAttention, feed_forward_block=FeedForward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention_block=self_attention_block\n",
    "        self.cross_attention_block=cross_attention_block\n",
    "        self.feed_forward_block=feed_forward_block\n",
    "        self.residual_connections=nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x=self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x=self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "758dba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__ (self, layers: nn.ModuleList()) -> None: \n",
    "        super().__init__()\n",
    "        self.layers=layers\n",
    "        self.norm=LayerNorm()\n",
    "        \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,encoder_output,src_mask,tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28a4663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self,d_model,vocab_size):\n",
    "        super().__init__()\n",
    "        self.linear=nn.Linear(d_model,vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Batch, seq_len, d_model--> Batch, seq_len, vocab_size\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a61175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__self(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed:InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding,projection_layer: ProjectionLayer):\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_embed=src_embed\n",
    "        self.tgt_embed=tgt_embed\n",
    "        self.src_pos=src_pos\n",
    "        self.tgt_pos=tgt_pos\n",
    "        self.projection_layer=projection_layer\n",
    "        \n",
    "    def encode(self, src, src_mask):\n",
    "        src=self.src_embed\n",
    "        src=self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
    "        tgt=self.tgt_embed\n",
    "        tgt=self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask) \n",
    "    \n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1da443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build(src_vocab_size, tgt_vocab_size, src_seq_len, tgt_seq_len, d_model=512, d_ff=2048, h=8, N=6, dropout=0.1, padding_idx=0):\n",
    "    #Create the embedding and positional encoding layers for both source and target\n",
    "    src_embed=InputEmbeddings(d_model,src_vocab_size,padding_idx)\n",
    "    tgt_embed=InputEmbeddings(d_model,tgt_vocab_size,padding_idx)\n",
    "    \n",
    "    src_pos=PositionalEncoding(d_model,src_seq_len,dropout)\n",
    "    tgt_pos=PositionalEncoding(d_model,tgt_seq_len,dropout)\n",
    "    \n",
    "    #Create the encoder and decoder blocks\n",
    "    encoder_blocks=[]\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention=MultiHeadAttention(d_model,h,dropout)\n",
    "        feed_forward_block=FeedForward(d_model,d_ff,dropout)\n",
    "        encoder_block=Encoder_Block(encoder_self_attention,feed_forward_block,dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "        \n",
    "    decoder_blocks=[]\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention=MultiHeadAttention(d_model,h,dropout)\n",
    "        cross_attention=MultiHeadAttention(d_model,h,dropout)\n",
    "        feed_forward_block=FeedForward(d_model,d_ff,dropout)\n",
    "        decoder_block=Decoder_Block(decoder_self_attention,cross_attention,feed_forward_block,dropout)\n",
    "        decoder_blocks.append(decoder_block) \n",
    "        \n",
    "    #Create Encoder and Decoder   \n",
    "    encoder=Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder=Decoder(nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    #Create the projection layer    \n",
    "    projection_layer=ProjectionLayer(d_model,tgt_vocab_size)\n",
    "    \n",
    "    transformer = Transformer(encoder,decoder,src_embed,tgt_embed,src_pos,tgt_pos,projection_layer)\n",
    "    #parameters initialize\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1f9a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\python_3\\lib\\site-packages (from tensorboard) (1.59.3)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading markdown-3.9-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\python_3\\lib\\site-packages (from tensorboard) (1.26.2)\n",
      "Requirement already satisfied: packaging in c:\\python_3\\lib\\site-packages (from tensorboard) (23.2)\n",
      "Requirement already satisfied: pillow in c:\\python_3\\lib\\site-packages (from tensorboard) (10.1.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\python_3\\lib\\site-packages (from tensorboard) (6.32.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\python_3\\lib\\site-packages (from tensorboard) (68.2.2)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python_3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.8/5.5 MB 3.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.6/5.5 MB 4.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.9/5.5 MB 4.5 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 3.7/5.5 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 4.7/5.5 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading markdown-3.9-py3-none-any.whl (107 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, markdown, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.3.1 markdown-3.9 tensorboard-2.20.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python_3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python_3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python_3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
